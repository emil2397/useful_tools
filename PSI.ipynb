{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emil2397/useful_tools/blob/main/PSI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-EiY7VLqbyY"
      },
      "source": [
        "# PSI index\n",
        "\n",
        "Ноутбук запускается по сегодняшней дате"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERqBmOdVqbyc"
      },
      "outputs": [],
      "source": [
        "# требуется utils\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"..\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3H_7oO-qbye"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3ynK5DJqbyf"
      },
      "outputs": [],
      "source": [
        "def load_large_data_to_pd2(spark, training_mode=False, base_path= '', nbatch = 400, core_feats = ['SupplierID', 'date'], load_feats = None):\n",
        "\n",
        "        start_time0 = time.time()\n",
        "\n",
        "        if training_mode:\n",
        "                sdf = spark.read.parquet(base_path).where(f\"...\")\n",
        "        else:\n",
        "                sdf = spark.read.parquet(base_path)\n",
        "\n",
        "        all_cols = sdf.columns\n",
        "\n",
        "        if(load_feats is not None):\n",
        "                all_cols = load_feats\n",
        "\n",
        "        tot_batch = len(all_cols)//nbatch + 1\n",
        "        res = []\n",
        "        start_time = time.time()\n",
        "        df = sdf.select(all_cols[:nbatch]).toPandas().sort_values(core_feats).reset_index(drop=True)\n",
        "        res.append(df)\n",
        "        print('join: step', 1, '/', tot_batch, df.shape, 'time: {0}s'.format(round( (time.time() - start_time), 1)))\n",
        "\n",
        "        for i in range(1,tot_batch):\n",
        "                gc.collect()\n",
        "                start_time = time.time()\n",
        "                df = sdf.select(core_feats + all_cols[i*nbatch:(i+1)*nbatch]).toPandas().sort_values(core_feats).reset_index(drop=True)\n",
        "                res.append(df.drop(core_feats, axis=1))\n",
        "                print('join: step', i+1, '/', tot_batch, df.shape, 'time: {0}s'.format(round( (time.time() - start_time), 1)))\n",
        "                del df\n",
        "\n",
        "        start_time = time.time()\n",
        "        df = pd.concat(res, axis=1)\n",
        "        del res\n",
        "        gc.collect()\n",
        "        print('concat', df.shape, 'time: {0}s'.format(round( (time.time() - start_time), 1)))\n",
        "        print('total time: {0}s'.format(round( (time.time() - start_time0), 1)))\n",
        "\n",
        "\n",
        "        return df\n",
        "\n",
        "def load_data(base_path, nbatch, dt1='2019-11-01', training_mode=False, base_features=None):\n",
        "\n",
        "    core_feats = [... ]\n",
        "\n",
        "    df = load_large_data_to_pd2(spark, training_mode, '...' + base_path +\"/...\", nbatch ,\n",
        "                              load_feats=base_features+core_feats\n",
        "                              )\n",
        "\n",
        "    gc.collect()\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    df = df[(df['date'] >= dt1)].copy()\n",
        "    df = df.sort_values(['date', 'SupplierID']).reset_index(drop=True)\n",
        "    df['ID'] = df.index.copy()\n",
        "    df['w'] = np.log(df['...'])\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4m3pmQAoqbyi"
      },
      "outputs": [],
      "source": [
        "def predict_new_data_ts(X, path):\n",
        "        \n",
        "        if path:\n",
        "            import subprocess\n",
        "            \n",
        "            m_f = os.path.join(os.getcwd(), 'models')\n",
        "            modelfolder_local = os.path.join(m_f, f\"{path.split('/')[-1]}\")\n",
        "            \n",
        "            if not os.path.exists(m_f):\n",
        "                os.mkdir(m_f)\n",
        "            \n",
        "            if not os.path.exists(modelfolder_local):\n",
        "                os.mkdir(modelfolder_local)\n",
        "            \n",
        "            seed_date = lambda x: [x.split('/')[-1].split('_')[0], '-'.join([x.split('/')[-1].split('_')[-3], x.split('/')[-1].split('_')[-1][:2], x.split('/')[-1].split('_')[-2]])]\n",
        "            \n",
        "            if not len(os.listdir(modelfolder_local))>0:\n",
        "\n",
        "                subprocess.run(['hdfs', 'dfs', '-get', '-f', path+'/*', modelfolder_local], capture_output=True,\n",
        "                    text=True).stdout\n",
        "            \n",
        "            model_names = [model_name for model_name in os.listdir(modelfolder_local)]\n",
        "\n",
        "            model_files = [os.path.join(modelfolder_local, f) for f in model_names if '.model' in f]\n",
        "            \n",
        "            s = sorted([seed_date(x) for x in model_files])\n",
        "            \n",
        "            model_files = sorted(model_files, key= lambda x:seed_date(x))\n",
        "            \n",
        "            max_date = s[-1][1]\n",
        "            min_date = s[0][1]\n",
        "            model_files = sorted(model_files, key= lambda x:seed_date(x))\n",
        "            tot_models = len(set([x[0] for x in s]))\n",
        "            \n",
        "            if tot_models >0:\n",
        "                    \n",
        "                    predict = np.zeros(shape=(X.shape[0],))\n",
        "                    features = list(pd.read_csv(os.path.join(modelfolder_local, 'train_features.txt')).columns)\n",
        "                    \n",
        "                    for i in range(len(model_files)):\n",
        "                        \n",
        "                            curr_seed, curr_date = s[i]\n",
        "                            \n",
        "                            model_loaded = lightgbm.Booster(model_file=model_files[i])\n",
        "                            \n",
        "                            \n",
        "                            if(curr_date!=max_date):\n",
        "                                    curr_ind = np.where(X['date'] == curr_date)[0]\n",
        "                            else:\n",
        "                                    curr_ind = np.where(X['date'] >= curr_date)[0]\n",
        "                                    \n",
        "                            if(X.iloc[curr_ind][features].shape[0]!=0):                     \n",
        "                                    predict[curr_ind] += model_loaded.predict(X.iloc[curr_ind][features].round(6)) / tot_models\n",
        "                                    \n",
        "                    null_index = np.where(X['date'] < min_date)\n",
        "                    predict[null_index] = -100\n",
        "                    \n",
        "                    return np.round(predict, decimals = 5)\n",
        "            else:\n",
        "                print('Models not found!!!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scQAFvTZqbyj"
      },
      "outputs": [],
      "source": [
        "def calculate_base_values(all_pds,sc):\n",
        "\n",
        "    def calc_bin_stats(df0, sc, npart=10 ):\n",
        "            df0.loc[:,'bin'],bins = pd.qcut(df0[sc],npart,duplicates='drop',retbins=True)\n",
        "            df = df0[[sc,'bin']].copy()\n",
        "            df['bin'] = df['bin'].apply(lambda x: x.left)\n",
        "            df['bin'] = df['bin'].astype(str)\n",
        "            df = df['bin'].value_counts().reset_index().rename({'index':'bin','bin':'counter_base'},axis=1)\n",
        "            df['base_prc'] = df['counter_base'] / sum(df['counter_base'])\n",
        "  \n",
        "\n",
        "            return df,bins\n",
        "        \n",
        "    def psi(new, base):\n",
        "        return round(sum((new-base) * np.log(new/base)),3)\n",
        "\n",
        "\n",
        "    score_names = []\n",
        "    bins_buckets = []\n",
        "    bins_used = []\n",
        "    base_prcs = []\n",
        "\n",
        "    base,bins = calc_bin_stats(all_pds.copy(), sc)\n",
        "\n",
        "    score_names.append(sc)\n",
        "    bins_buckets.append(list(base['bin'].values))\n",
        "    base_prcs.append(list(base['base_prc'].values))\n",
        "    bins_used.append(list(bins))\n",
        "\n",
        "    params = tuple([list(base['bin'].values), list(base['base_prc'].values), list(bins_used[0])])\n",
        "    \n",
        "    return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "0cQxgeW-qbyk"
      },
      "outputs": [],
      "source": [
        "spark = run_spark(...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBQxTsAAqbyk"
      },
      "source": [
        "# Расчет словарей pd/feats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGpGoM7Tqbyl"
      },
      "outputs": [],
      "source": [
        "# ! pip install pydoop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56lDqv93qbyl"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUl6RdS7qbym"
      },
      "outputs": [],
      "source": [
        "def calculate_base_stats(model_folder,models_names,df_name, base_start_date, base_end_date):\n",
        "\n",
        "    feats_all = []\n",
        "\n",
        "    for model in models_names:\n",
        "        with hd.open(f\"{model_folder}/{model}/train_features.txt\") as f:\n",
        "            feats_all=feats_all+ list(pd.read_csv(f).columns)\n",
        "\n",
        "    feats=list(set(feats_all))\n",
        "    \n",
        "    df = load_data(df_name, 500, base_start_date, training_mode=True, base_features=feats)\n",
        "    \n",
        "    print(\"1 LOAD OK!\")\n",
        "    \n",
        "    for model in tqdm(models_names):\n",
        "        df[model] = predict_new_data_ts(df, model_folder+f'/{model}')\n",
        "\n",
        "    df['pd'] = df[models_names].mean(axis=1)\n",
        "    \n",
        "    print(\"2 PD OK!\")\n",
        "    \n",
        "    base_mask = (df[\"date\"] >= base_start_date) & (df[\"date\"] <= base_end_date) & (df[\"...\"])\n",
        "    base_bins_feats = {}\n",
        "    feats_extended = [\"pd\"] + feats\n",
        "\n",
        "    for col in feats_extended:\n",
        "        base_bins_feats[col] = calculate_base_values(df[base_mask], sc=col)\n",
        "    \n",
        "    print(\"3 DICTS OK!\")\n",
        "    \n",
        "    return df, base_bins_feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "VKG8sZddqbyn",
        "outputId": "8e689915-d710-44ed-b907-2b6c15e1f090"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "join: step 1 / 1 (146188, 148) time: 34.9s\n",
            "concat (146188, 148) time: 0.3s\n",
            "total time: 69.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/eyusifov@ozon/python3_env/lib/python3.7/site-packages/pandas/core/arraylike.py:358: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 LOAD OK!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [06:24<00:00, 96.22s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2 PD OK!\n",
            "3 DICTS OK!\n"
          ]
        }
      ],
      "source": [
        "if (0):\n",
        "    df_gen, base_bins_feats = calculate_base_stats(model_folder,models_names,df_name,base_start_date, base_end_date)\n",
        "    \n",
        "    with open('config.json', 'wb') as fp:\n",
        "        pickle.dump(base_bins_feats, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IYaqGr2Yqbyo"
      },
      "outputs": [],
      "source": [
        "# with open('config.json', 'rb') as fp:\n",
        "#     base_bins_feats = pickle.load(fp)\n",
        "\n",
        "# print(base_bins_feats)\n",
        "# print(type(base_bins_feats))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wlXak3_qbyp"
      },
      "source": [
        "## Расчет тестовых"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-WiHCJfqbyp"
      },
      "outputs": [],
      "source": [
        "def test_psi(spark, base_bins_feats, test_path_read, only_critical=False, part_filter='', day=None, logger=print):\n",
        "    \n",
        "    def calculate_psi_index(params,  feat, df_test=None, npart=10):\n",
        "\n",
        "        def psi(new, base):\n",
        "            return round(sum((new-base) * np.log(new/base)),3)\n",
        "\n",
        "        bin_df,base_prc,bins = params\n",
        "        base_res = pd.DataFrame({'bin':bin_df,'base_prc':base_prc})\n",
        "        df = df_test.copy()   \n",
        "        df.loc[:,'bin'] = pd.cut(df[feat],bins=bins,include_lowest=True)\n",
        "        upper_bounds = {}\n",
        "\n",
        "        for i,lower_bound in enumerate(bins):\n",
        "            if i!=len(bins)-1:\n",
        "                upper_bounds[round(lower_bound,2)] = round(bins[i+1],4)\n",
        "            else:\n",
        "                upper_bounds[round(lower_bound,2)] = 10000.\n",
        "\n",
        "        df.loc[:,'bin_upper'] = df['bin'].apply(lambda x: x.right)\n",
        "        df.loc[:,'bin_upper'] = df['bin_upper'].astype(str)\n",
        "\n",
        "        df.loc[:,'bin'] = df['bin'].apply(lambda x: x.left)\n",
        "        df.loc[:,'bin'] = df['bin'].astype(str)\n",
        "\n",
        "\n",
        "        df_res = df['bin'].value_counts().reset_index().rename({'index':'bin','bin':'counter_current'},axis=1).copy()\n",
        "        df_res['bin_prc'] = df_res['counter_current'] / sum(df_res['counter_current'])\n",
        "        base_res['bin'] = base_res['bin'].astype(str)    \n",
        "        res = pd.merge(df_res,base_res,on='bin',how='outer')\n",
        "        ub = df[['bin','bin_upper']].groupby('bin').first().reset_index()\n",
        "        res = pd.merge(res,ub,on='bin',how='inner')\n",
        "        res.drop('bin',axis=1,inplace=True)\n",
        "        res.rename({'bin_upper':'bin'},axis=1,inplace=True)\n",
        "        res = res.sort_values(by=['bin'],ascending=False,axis=0,na_position='first')\n",
        "        res['bin'] = res['bin'].astype(float).round(2)\n",
        "\n",
        "        return psi(res['bin_prc'],res['base_prc']),res\n",
        "    \n",
        "\n",
        "    test_path_read = test_path_read.format(day.strftime('%Y%m%d'))\n",
        "\n",
        "    feats_load = list(base_bins_feats.keys())\n",
        "\n",
        "    print(f\"DATE of scoring: {datetime.now().strftime('%Y-%m-%d')}\")\n",
        "    \n",
        "        \n",
        "    print(f\"DATE of file: {day.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "    if len(part_filter)==0:\n",
        "        df_test = spark.read.parquet(test_path_read).toPandas()[feats_load]\n",
        "    else:\n",
        "        df_test = spark.read.parquet(test_path_read).where(f\"{part_filter}\").toPandas()[feats_load]\n",
        "    \n",
        "    output = {}\n",
        "    \n",
        "    for feat in base_bins_feats.keys():\n",
        "        output[feat] = calculate_psi_index(params=base_bins_feats[feat], feat=feat, df_test=df_test)[0]\n",
        "    \n",
        "    output_df = pd.DataFrame.from_dict(output, orient='index')\n",
        "    output_df = output_df.rename(columns={0:'psi_value'})\n",
        "    output_df[\"psi_value\"] = output_df[\"psi_value\"].round(4)    \n",
        "\n",
        "\n",
        "    if (output_df['psi_value'] > 0.25).any() & only_critical:\n",
        "        output_df=output_df[output_df['psi_value'] > 0.25]\n",
        "        logger(f\"YES CRITICAL PSI: \\n {msg}\")\n",
        "        \n",
        "    else:\n",
        "        if (output_df['psi_value'] > 0.25).any():\n",
        "            logger(f\"YES CRITICAL PSI\")\n",
        "        else:\n",
        "            logger(f\"NO CRITICAL PSI\")\n",
        "        \n",
        "    \n",
        "    return output_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1yK6cL-qbyq"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'sellers' : {\"base_bins_feats\": base_bins_feats, \n",
        "                \"test_path_read\": '...',\n",
        "                \"only_critical\": False,\n",
        "                \"part_filter\": \"...\"\n",
        "                },\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "TFaGRTh3qbyr"
      },
      "outputs": [],
      "source": [
        "import datetime as DT\n",
        "today = DT.date.today()\n",
        "week_ago = today - DT.timedelta(days=7)\n",
        "\n",
        "dates = {}\n",
        "\n",
        "for day in pd.date_range(week_ago,today):\n",
        "    \n",
        "    dates[day.strftime(\"%Y-%m-%d\")] = test_psi(spark=spark, day=day, **config[\"sellers\"]).to_dict()[\"psi_value\"]\n",
        "                                                \n",
        "    print(\"\\n\")\n",
        "    \n",
        "result=pd.DataFrame.from_dict(dates, orient='columns')\n",
        "result=result.sort_values(by=result.columns[-1],ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyPJKM40qbys"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPl2sfVyqbys"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBiU10-Pqbyt"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5jVgkmNqbyt"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "yjeP3yMSqbyv"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWJVU9Itqbyv"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCxETZ38qbyv"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python3_env",
      "language": "python",
      "name": "python3_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": false,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": false,
      "toc_window_display": false
    },
    "colab": {
      "name": "PSI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}